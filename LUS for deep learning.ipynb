{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import tensorflow as tf\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bernoulli(p):\n",
    "    np.random.seed(int(time.time()))\n",
    "    if np.random.random()<p :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "def get_Q(P_xy):\n",
    "    \"\"\"\n",
    "    P_xy is the K*n matrix indicating probability p(xi,yi=k)\n",
    "    return Q is n*1 matrix indicating qi=max(0.5, p(i,1),..., p(i,K))\n",
    "    \n",
    "    \"\"\"\n",
    "    K, n = np.shape(P_xy)\n",
    "    Q = np.max(P_xy, axis=0)\n",
    "    Q[Q<0.5]=0.5\n",
    "    return Q\n",
    "\n",
    "\n",
    "def get_A(P_xy, gamma):\n",
    "    \"\"\"\n",
    "    gamma>1 is a parameter\n",
    "    return A K*n is the sampling probability 改成n*K?\n",
    "    \n",
    "    \"\"\"\n",
    "    K, n = np.shape(P_xy)\n",
    "    Q=get_Q(P_xy)\n",
    "    a = 2*Q/gamma\n",
    "    a[a>1]=1\n",
    "    A=np.matrix([a, ]* K)\n",
    "    A=np.asarray(A)\n",
    "    idx=np.argmax(P_xy, axis=0)#每一列最大值\n",
    "    for i in range(0, n):\n",
    "            if(P_xy[idx[i], i]>=0.5):\n",
    "                A[idx[i], i]=(1-Q[i])/(gamma-np.max([Q[i], 0.5*gamma]))\n",
    "    return A.T\n",
    "\n",
    "\n",
    "def get_Z(P_xy, Y, gamma):\n",
    "    \"\"\"\n",
    "    sample zi~Bernoulli(a(xi, yi): zi=1)\n",
    "    class label Y is needed, each value yi is in [0, K-1]\n",
    "    return Z n*1 {0,1} matrix indicating whether to use that sample\n",
    "    \n",
    "    \"\"\"\n",
    "    n, = Y.shape\n",
    "    A = get_A(P_xy, gamma)\n",
    "    Z = np.zeros(n)\n",
    "    i=0\n",
    "    for i in range(0,n):\n",
    "        Z[i]=Bernoulli(A[i, int(Y[i])])\n",
    "        \n",
    "    return Z\n",
    "\n",
    "\n",
    "def get_sample(X, Y, P_xy, gamma):\n",
    "    \"\"\"\n",
    "    subsample X and Y to get training sample and also the corresponding A\n",
    "    X is n*p\n",
    "    y is n\n",
    "    \n",
    "    \"\"\"\n",
    "    Z = get_Z(P_xy, Y, gamma)\n",
    "    X_sample = X[Z==1, :]\n",
    "    Y_sample = np.array(Y)[Z==1]\n",
    "    A = get_A(P_xy, gamma)\n",
    "    A_sample = A[Z==1, :]\n",
    "    return X_sample, Y_sample, A_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Network architecture\n",
    "input -> 5*5*32convolution -> 2*2 max pooling -> 5*5*64 convolution -> 2*2 max pooling -> 2*2*1024 -> ReLu -> Likelihood function\n",
    "\n",
    "\"\"\"\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "A = tf.placeholder(tf.float32, [None, num_classes])  # 我们自己的把A加进去\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='VALID')\n",
    "# 创建模型\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32],mean=0,stddev=0.1)),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64],mean=0,stddev=0.1)),\n",
    "    # fully connected, 4*4*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([4 * 4 * 64, 1024],mean=0,stddev=0.1)),\n",
    "    # 1024 inputs, 9 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, num_classes - 1],mean=0,stddev=0.1))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32],mean=0,stddev=0.1)),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64],mean=0,stddev=0.1)),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([1024],mean=0,stddev=0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([num_classes - 1],mean=0,stddev=0.1))\n",
    "} \n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)  # (batch_Size, K-1)\n",
    "G = tf.concat([logits, tf.cast(np.zeros((batch_size, 1)), tf.float32)], axis=1)\n",
    "temp1 = tf.multiply(G, Y)\n",
    "temp1 = tf.reduce_sum(temp1, axis=1)\n",
    "temp1 = tf.reduce_mean(temp1)\n",
    "temp2 = tf.exp(logits)\n",
    "temp2 = tf.reduce_sum(temp2, axis=1)\n",
    "temp2 = tf.add(temp2, 1.0)\n",
    "temp2 = tf.log(temp2)\n",
    "temp2 = tf.reduce_mean(temp2)\n",
    "loss_op = tf.subtract(temp1, temp2)\n",
    "loss_op = tf.subtract(0.0, loss_op)\n",
    "pred = tf.argmax(G, 1)\n",
    "# Define loss and optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(pred, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x171e9f8be10>]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##测试一下gamma与sample数量的关系\n",
    "#val_label永远保持非one hot的形式，用于训练取样，后来的y_val为one hot形式， 标记一下\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=False)\n",
    "val_data = mnist.validation.images  #(5000,784)\n",
    "val_labels = mnist.validation.labels\n",
    "train_labels = mnist.train.labels\n",
    "train_data = mnist.train.images\n",
    "#####\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "X_val = mnist.validation.images  #(5000,784)\n",
    "y_val = mnist.validation.labels\n",
    "y_val = np.eye(num_classes)[y_val]\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(val_data, val_labels)\n",
    "P_xy = classifier.predict_proba(train_data)\n",
    "P_xy = P_xy.T\n",
    "gamma_range = [1.1, 1.2, 1.3, 1.5, 1.8, 2, 2.5, 3, 5, 8, 10]\n",
    "sample_fraction = np.zeros(11)\n",
    "for i in range(11):\n",
    "    X_sample, y_sample, A_sample = get_sample(train_data, train_labels, P_xy, gamma_range[i])\n",
    "    sample_fraction[i] = y_sample.shape[0]/train_labels.shape[0]\n",
    "    \n",
    "plt.plot(gamma_range, sample_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 55000)\n",
      "accuracy is:  0.8907636363636364\n",
      "# of sample is:  11747\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(val_data, val_labels)\n",
    "P_xy = classifier.predict_proba(train_data)\n",
    "P_xy = P_xy.T\n",
    "print(P_xy.shape)\n",
    "print(\"accuracy is: \",(1-sum(classifier.predict(train_data)!=train_labels)/train_nums))\n",
    "\n",
    "X_sample, y_sample, A_sample = get_sample(train_data, train_labels, P_xy, gamma)\n",
    "#print(y_sample[:10])\n",
    "y_sample = np.eye(num_classes)[y_sample]#数字编码 to one hot, 牛逼\n",
    "\n",
    "\n",
    "print(\"# of sample is: \", y_sample.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005  # 学习率\n",
    "batch_size = 128  # 批大小\n",
    "num_steps = 600  # 使用的样本数量\n",
    "display_step = 50  # 显示间隔\n",
    "num_input = 784  # image shape:28*28\n",
    "num_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # 用于随机丢弃，防止过拟\n",
    "\n",
    "def get_batch_from_X(X, y, A, batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch and return the corresponding X_batch, y_batch, A_batch\n",
    "    \"\"\"\n",
    "    data_size = X.shape[0]\n",
    "    idx = np.random.permutation(data_size)\n",
    "    X_random = X[idx]\n",
    "    X_batch = X_random[:batch_size, :]\n",
    "    y_random = y[idx]\n",
    "    y_batch = y_random[:batch_size, :]\n",
    "    A_random = A[idx]\n",
    "    A_batch = A_random[:batch_size, :]\n",
    "\n",
    "    return X_batch, y_batch, A_batch\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y, batch_A = get_batch_from_X(X_sample, y_sample, A_sample, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op,feed_dict={X: batch_x, Y: batch_y, A: batch_A, keep_prob: dropout})\n",
    "        #print(np.shape(batch_x),np.shape(batch_y))\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,Y: batch_y,A: batch_A, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=3.9860, Training Accuracy=0.133\n",
      "Step 50, Minibatch Loss=0.4973, Training Accuracy=0.852\n",
      "Step 100, Minibatch Loss=0.2231, Training Accuracy=0.930\n",
      "Step 150, Minibatch Loss=0.1481, Training Accuracy=0.953\n",
      "Step 200, Minibatch Loss=0.1989, Training Accuracy=0.969\n",
      "Step 250, Minibatch Loss=0.0709, Training Accuracy=0.969\n",
      "Step 300, Minibatch Loss=0.0460, Training Accuracy=0.992\n",
      "Step 350, Minibatch Loss=0.0380, Training Accuracy=0.992\n",
      "Step 400, Minibatch Loss=0.0705, Training Accuracy=0.984\n",
      "Step 450, Minibatch Loss=0.0169, Training Accuracy=1.000\n",
      "Step 500, Minibatch Loss=0.0224, Training Accuracy=1.000\n",
      "Step 550, Minibatch Loss=0.0330, Training Accuracy=0.992\n",
      "Step 600, Minibatch Loss=0.0288, Training Accuracy=0.992\n",
      "test accuracy over 20 batches is : 0.9859375\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                        A: np.ones([batch_size, num_classes]), keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n接下来要做一些实验了，\\n1.测试一下这个A是不是有用，尝试一次用np.ones训练\\n2.测试一下取出来的sample在softmax的网络上好不好用\\n3.对比一下uniform sample, 为了省事直接用test_set代替unifor sample的set\\n\\n'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "接下来要做一些实验了，\n",
    "1.测试一下这个A是不是有用，尝试一次用np.ones训练\n",
    "2.测试一下取出来的sample在softmax的网络上好不好用\n",
    "3.对比一下uniform sample, 为了省事直接用test_set代替unifor sample的set\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 55000)\n",
      "accuracy is:  0.8907636363636364\n",
      "# of sample is:  8542\n"
     ]
    }
   ],
   "source": [
    "#首先测试A为1的情况\n",
    "gamma = 2\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(val_data, val_labels)\n",
    "P_xy = classifier.predict_proba(train_data)\n",
    "P_xy = P_xy.T\n",
    "print(P_xy.shape)\n",
    "print(\"accuracy is: \",(1-sum(classifier.predict(train_data)!=train_labels)/train_nums))\n",
    "\n",
    "X_sample, y_sample, A_sample = get_sample(train_data, train_labels, P_xy, gamma)\n",
    "##改动\n",
    "A_sample = np.ones_like(A_sample)\n",
    "#print(y_sample[:10])\n",
    "y_sample = np.eye(num_classes)[y_sample]#数字编码 to one hot, 牛逼\n",
    "\n",
    "\n",
    "print(\"# of sample is: \", y_sample.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=4.0843, Training Accuracy=0.203\n",
      "Step 50, Minibatch Loss=0.6068, Training Accuracy=0.820\n",
      "Step 100, Minibatch Loss=0.1937, Training Accuracy=0.938\n",
      "Step 150, Minibatch Loss=0.1270, Training Accuracy=0.977\n",
      "Step 200, Minibatch Loss=0.1152, Training Accuracy=0.953\n",
      "Step 250, Minibatch Loss=0.0626, Training Accuracy=0.984\n",
      "Step 300, Minibatch Loss=0.0738, Training Accuracy=0.977\n",
      "Step 350, Minibatch Loss=0.0375, Training Accuracy=0.984\n",
      "Step 400, Minibatch Loss=0.0232, Training Accuracy=1.000\n",
      "Step 450, Minibatch Loss=0.0544, Training Accuracy=0.984\n",
      "Step 500, Minibatch Loss=0.0654, Training Accuracy=0.977\n",
      "Step 550, Minibatch Loss=0.0295, Training Accuracy=0.992\n",
      "Step 600, Minibatch Loss=0.0151, Training Accuracy=1.000\n",
      "test accuracy over 20 batches is : 0.98046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n结论，把训练中的权重A去掉似乎没什么大的影响，应该是因为大部分分类都是准确的，在准确的情况下其他类概率极低\\n加不加权意义不大\\n'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                        A: np.ones([batch_size, num_classes]), keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))\n",
    "\"\"\"\n",
    "结论，把训练中的权重A去掉似乎没什么大的影响，应该是因为大部分分类都是准确的，在准确的情况下其他类概率极低\n",
    "加不加权意义不大\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=3.1488, Training Accuracy=0.328\n",
      "Step 50, Minibatch Loss=0.1469, Training Accuracy=0.953\n",
      "Step 100, Minibatch Loss=0.1087, Training Accuracy=0.961\n",
      "Step 150, Minibatch Loss=0.0431, Training Accuracy=0.984\n",
      "Step 200, Minibatch Loss=0.0669, Training Accuracy=0.969\n",
      "Step 250, Minibatch Loss=0.0057, Training Accuracy=1.000\n",
      "Step 300, Minibatch Loss=0.0067, Training Accuracy=1.000\n",
      "Step 350, Minibatch Loss=0.0130, Training Accuracy=1.000\n",
      "Step 400, Minibatch Loss=0.0027, Training Accuracy=1.000\n",
      "Step 450, Minibatch Loss=0.0018, Training Accuracy=1.000\n",
      "Step 500, Minibatch Loss=0.0154, Training Accuracy=0.992\n",
      "Step 550, Minibatch Loss=0.0112, Training Accuracy=0.992\n",
      "Step 600, Minibatch Loss=0.0044, Training Accuracy=1.000\n",
      "test accuracy over 20 batches is : 0.98046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n这种情况我们称之为没有什么显著区别\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对比一下随机取样的, 用测试集的1w个样本代替随机抽样\n",
    "#用val_data来做验证\n",
    "\n",
    "\n",
    "A_s = np.ones([10000, num_classes])\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y, batch_A = get_batch_from_X(X_test, y_test, A_s, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op,feed_dict={X: batch_x, Y: batch_y, A: batch_A, keep_prob: dropout})\n",
    "        #print(np.shape(batch_x),np.shape(batch_y))\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,Y: batch_y,A: batch_A, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "\n",
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_val[i*batch_size : (i+1)*batch_size,:],Y: y_val[i*batch_size : (i+1)*batch_size,:], \n",
    "                                        A: np.ones([batch_size, num_classes]), keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))\n",
    "    \n",
    "\"\"\"\n",
    "这种情况我们称之为没有什么显著区别\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后再来测试一下softmax的普通网络,首先是以刚才的sample为data\n",
    "\n",
    "learning_rate = 0.001 #学习率 \n",
    "batch_size = 128 #批大小\n",
    "num_steps = 500 #使用的样本数量\n",
    "display_step = 50 #显示间隔\n",
    "\n",
    "num_input = 784 #image shape:28*28\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 #用于随机丢弃，防止过拟\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "#创建模型\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "pred = tf.argmax(prediction, 1)\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(pred, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1,num_steps+1):\n",
    "        batch_x, batch_y, A_ = get_batch_from_X(X_sample, y_sample, A_sample, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=35831.0156, Training Accuracy=0.133\n",
      "Step 50, Minibatch Loss=8594.5166, Training Accuracy=0.359\n",
      "Step 100, Minibatch Loss=3635.9116, Training Accuracy=0.602\n",
      "Step 150, Minibatch Loss=3565.2568, Training Accuracy=0.625\n",
      "Step 200, Minibatch Loss=1438.3538, Training Accuracy=0.789\n",
      "Step 250, Minibatch Loss=887.3711, Training Accuracy=0.844\n",
      "Step 300, Minibatch Loss=587.2196, Training Accuracy=0.828\n",
      "Step 350, Minibatch Loss=1117.1187, Training Accuracy=0.805\n",
      "Step 400, Minibatch Loss=686.4462, Training Accuracy=0.883\n",
      "Step 450, Minibatch Loss=644.5966, Training Accuracy=0.875\n",
      "Step 500, Minibatch Loss=626.5801, Training Accuracy=0.836\n",
      "test accuracy over 20 batches is : 0.899609375\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                         keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))\n",
    "\"\"\"\n",
    "我们的sample在softmax的cnn上效果不是很好\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8542, 784)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=58786.6641, Training Accuracy=0.109\n",
      "Step 50, Minibatch Loss=2919.4658, Training Accuracy=0.820\n",
      "Step 100, Minibatch Loss=2419.5137, Training Accuracy=0.875\n",
      "Step 150, Minibatch Loss=1421.5408, Training Accuracy=0.891\n",
      "Step 200, Minibatch Loss=1384.2700, Training Accuracy=0.914\n",
      "Step 250, Minibatch Loss=746.1592, Training Accuracy=0.945\n",
      "Step 300, Minibatch Loss=781.3927, Training Accuracy=0.945\n",
      "Step 350, Minibatch Loss=541.2388, Training Accuracy=0.977\n",
      "Step 400, Minibatch Loss=486.9713, Training Accuracy=0.938\n",
      "Step 450, Minibatch Loss=986.3697, Training Accuracy=0.938\n",
      "Step 500, Minibatch Loss=541.0728, Training Accuracy=0.969\n",
      "test accuracy over 20 batches is : 0.93203125\n"
     ]
    }
   ],
   "source": [
    "##最后来测试一下softmax方法在整个train_data上的表现\n",
    "\n",
    "def get_batch_from_X1(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch and return the corresponding X_batch, y_batch, A_batch\n",
    "    \"\"\"\n",
    "    data_size = X.shape[0]\n",
    "    idx = np.random.permutation(data_size)\n",
    "    X_random = X[idx]\n",
    "    X_batch = X_random[:batch_size, :]\n",
    "    y_random = y[idx]\n",
    "    y_batch = y_random[:batch_size, :]\n",
    "\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1,num_steps+1):\n",
    "        batch_x, batch_y = get_batch_from_X1(X_train, y_train, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n",
    "            \n",
    "            \n",
    "            \n",
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                         keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
