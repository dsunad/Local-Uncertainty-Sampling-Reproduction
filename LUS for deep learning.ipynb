{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import tensorflow as tf\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bernoulli(p):\n",
    "    np.random.seed(int(time.time()))\n",
    "    if np.random.random()<p :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "def get_Q(P_xy):\n",
    "    \"\"\"\n",
    "    P_xy is the K*n matrix indicating probability p(xi,yi=k)\n",
    "    return Q is n*1 matrix indicating qi=max(0.5, p(i,1),..., p(i,K))\n",
    "    \n",
    "    \"\"\"\n",
    "    K, n = np.shape(P_xy)\n",
    "    Q = np.max(P_xy, axis=0)\n",
    "    Q[Q<0.5]=0.5\n",
    "    return Q\n",
    "\n",
    "\n",
    "def get_A(P_xy, gamma):\n",
    "    \"\"\"\n",
    "    gamma>1 is a parameter\n",
    "    return A K*n is the sampling probability 改成n*K?\n",
    "    \n",
    "    \"\"\"\n",
    "    K, n = np.shape(P_xy)\n",
    "    Q=get_Q(P_xy)\n",
    "    a = 2*Q/gamma\n",
    "    a[a>1]=1\n",
    "    A=np.matrix([a, ]* K)\n",
    "    A=np.asarray(A)\n",
    "    idx=np.argmax(P_xy, axis=0)#每一列最大值\n",
    "    for i in range(0, n):\n",
    "            if(P_xy[idx[i], i]>=0.5):\n",
    "                A[idx[i], i]=(1-Q[i])/(gamma-np.max([Q[i], 0.5*gamma]))\n",
    "    return A.T\n",
    "\n",
    "\n",
    "def get_Z(P_xy, Y, gamma):\n",
    "    \"\"\"\n",
    "    sample zi~Bernoulli(a(xi, yi): zi=1)\n",
    "    class label Y is needed, each value yi is in [0, K-1]\n",
    "    return Z n*1 {0,1} matrix indicating whether to use that sample\n",
    "    \n",
    "    \"\"\"\n",
    "    n, = Y.shape\n",
    "    A = get_A(P_xy, gamma)\n",
    "    Z = np.zeros(n)\n",
    "    i=0\n",
    "    for i in range(0,n):\n",
    "        Z[i]=Bernoulli(A[i, int(Y[i])])\n",
    "        \n",
    "    return Z\n",
    "\n",
    "\n",
    "def get_sample(X, Y, P_xy, gamma):\n",
    "    \"\"\"\n",
    "    subsample X and Y to get training sample and also the corresponding A\n",
    "    X is n*p\n",
    "    y is n\n",
    "    \n",
    "    \"\"\"\n",
    "    Z = get_Z(P_xy, Y, gamma)\n",
    "    X_sample = X[Z==1, :]\n",
    "    Y_sample = np.array(Y)[Z==1]\n",
    "    A = get_A(P_xy, gamma)\n",
    "    A_sample = A[Z==1, :]\n",
    "    return X_sample, Y_sample, A_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-7aba8c4efa7a>:43: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Network architecture\n",
    "input -> 5*5*32convolution -> 2*2 max pooling -> 5*5*64 convolution -> 2*2 max pooling -> 2*2*1024 -> ReLu -> Likelihood function\n",
    "\n",
    "\"\"\"\n",
    "learning_rate = 0.0005  # 学习率\n",
    "batch_size = 128  # 批大小\n",
    "num_steps = 600  # 使用的样本数量\n",
    "display_step = 50  # 显示间隔\n",
    "num_input = 784  # image shape:28*28\n",
    "num_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # 用于随机丢弃，防止过拟\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "A = tf.placeholder(tf.float32, [None, num_classes])  # 我们自己的把A加进去\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='VALID')\n",
    "# 创建模型\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32],mean=0,stddev=0.1)),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64],mean=0,stddev=0.1)),\n",
    "    # fully connected, 4*4*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([4 * 4 * 64, 1024],mean=0,stddev=0.1)),\n",
    "    # 1024 inputs, 9 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, num_classes - 1],mean=0,stddev=0.1))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32],mean=0,stddev=0.1)),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64],mean=0,stddev=0.1)),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([1024],mean=0,stddev=0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([num_classes - 1],mean=0,stddev=0.1))\n",
    "} \n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)  # (batch_Size, K-1)\n",
    "G = tf.concat([logits, tf.cast(np.zeros((batch_size, 1)), tf.float32)], axis=1)\n",
    "temp1 = tf.multiply(G, Y)\n",
    "temp1 = tf.reduce_sum(temp1, axis=1)\n",
    "temp1 = tf.reduce_mean(temp1)\n",
    "temp2 = tf.exp(logits)\n",
    "temp2 = tf.reduce_sum(temp2, axis=1)\n",
    "temp2 = tf.add(temp2, 1.0)\n",
    "temp2 = tf.log(temp2)\n",
    "temp2 = tf.reduce_mean(temp2)\n",
    "loss_op = tf.subtract(temp1, temp2)\n",
    "loss_op = tf.subtract(0.0, loss_op)\n",
    "pred = tf.argmax(G, 1)\n",
    "# Define loss and optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(pred, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b4bf7f1278>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##测试一下gamma与sample数量的关系\n",
    "#val_label永远保持非one hot的形式，用于训练取样，后来的y_val为one hot形式， 标记一下\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=False)\n",
    "val_data = mnist.validation.images  #(5000,784)\n",
    "val_labels = mnist.validation.labels\n",
    "train_labels = mnist.train.labels\n",
    "train_data = mnist.train.images\n",
    "#####\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "X_val = mnist.validation.images  #(5000,784)\n",
    "y_val = mnist.validation.labels\n",
    "y_val = np.eye(num_classes)[y_val]\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(val_data, val_labels)\n",
    "P_xy = classifier.predict_proba(train_data)\n",
    "P_xy = P_xy.T\n",
    "gamma_range = [1.1, 1.2, 1.3, 1.5, 1.8, 2, 2.5, 3, 5, 8, 10]\n",
    "sample_fraction = np.zeros(11)\n",
    "for i in range(11):\n",
    "    X_sample, y_sample, A_sample = get_sample(train_data, train_labels, P_xy, gamma_range[i])\n",
    "    sample_fraction[i] = y_sample.shape[0]/train_labels.shape[0]\n",
    "    \n",
    "plt.plot(gamma_range, sample_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 55000)\n",
      "accuracy is:  0.8907636363636364\n",
      "# of sample is:  8741\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(val_data, val_labels)\n",
    "P_xy = classifier.predict_proba(train_data)\n",
    "P_xy = P_xy.T\n",
    "print(P_xy.shape)\n",
    "print(\"accuracy is: \",(1-sum(classifier.predict(train_data)!=train_labels)/train_data.shape[0]))\n",
    "\n",
    "X_sample, y_sample, A_sample = get_sample(train_data, train_labels, P_xy, gamma)\n",
    "#print(y_sample[:10])\n",
    "y_sample = np.eye(num_classes)[y_sample]#数字编码 to one hot, 牛逼\n",
    "\n",
    "\n",
    "print(\"# of sample is: \", y_sample.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002  # 学习率\n",
    "batch_size = 128  # 批大小\n",
    "num_steps = 500  # 使用的样本数量\n",
    "display_step = 50  # 显示间隔\n",
    "num_input = 784  # image shape:28*28\n",
    "num_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # 用于随机丢弃，防止过拟\n",
    "\n",
    "def get_batch_from_X(X, y, A, batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch and return the corresponding X_batch, y_batch, A_batch\n",
    "    \"\"\"\n",
    "    data_size = X.shape[0]\n",
    "    idx = np.random.permutation(data_size)\n",
    "    X_random = X[idx]\n",
    "    X_batch = X_random[:batch_size, :]\n",
    "    y_random = y[idx]\n",
    "    y_batch = y_random[:batch_size, :]\n",
    "    A_random = A[idx]\n",
    "    A_batch = A_random[:batch_size, :]\n",
    "\n",
    "    return X_batch, y_batch, A_batch\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y, batch_A = get_batch_from_X(X_sample, y_sample, A_sample, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op,feed_dict={X: batch_x, Y: batch_y, A: batch_A, keep_prob: dropout})\n",
    "        #print(np.shape(batch_x),np.shape(batch_y))\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,Y: batch_y,A: batch_A, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=3.3199, Training Accuracy=0.133\n",
      "Step 50, Minibatch Loss=0.8866, Training Accuracy=0.750\n",
      "Step 100, Minibatch Loss=0.4116, Training Accuracy=0.859\n",
      "Step 150, Minibatch Loss=0.3175, Training Accuracy=0.922\n",
      "Step 200, Minibatch Loss=0.2229, Training Accuracy=0.906\n",
      "Step 250, Minibatch Loss=0.2200, Training Accuracy=0.922\n",
      "Step 300, Minibatch Loss=0.1557, Training Accuracy=0.953\n",
      "Step 350, Minibatch Loss=0.2202, Training Accuracy=0.953\n",
      "Step 400, Minibatch Loss=0.1063, Training Accuracy=0.977\n",
      "Step 450, Minibatch Loss=0.0862, Training Accuracy=0.984\n",
      "Step 500, Minibatch Loss=0.0533, Training Accuracy=0.984\n",
      "test accuracy over 20 batches is : 0.9828125\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                        A: np.ones([batch_size, num_classes]), keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n接下来要做一些实验了，\\n1.测试一下这个A是不是有用，尝试一次用np.ones训练\\n2.测试一下取出来的sample在softmax的网络上好不好用\\n3.对比一下uniform sample, 为了省事直接用test_set代替unifor sample的set\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "接下来要做一些实验了，\n",
    "1.测试一下这个A是不是有用，尝试一次用np.ones训练\n",
    "2.测试一下取出来的sample在softmax的网络上好不好用\n",
    "3.对比一下uniform sample, 为了省事直接用test_set代替unifor sample的set\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Misika Mei\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 55000)\n",
      "accuracy is:  0.8907636363636364\n",
      "# of sample is:  12785\n"
     ]
    }
   ],
   "source": [
    "#首先测试A为1的情况\n",
    "gamma = 1.5\n",
    "train_nums = X_train.shape[0]\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(val_data, val_labels)\n",
    "P_xy = classifier.predict_proba(train_data)\n",
    "P_xy = P_xy.T\n",
    "print(P_xy.shape)\n",
    "print(\"accuracy is: \",(1-sum(classifier.predict(train_data)!=train_labels)/train_nums))\n",
    "\n",
    "X_sample, y_sample, A_sample = get_sample(train_data, train_labels, P_xy, gamma)\n",
    "##改动\n",
    "A_sample = np.ones_like(A_sample)\n",
    "#print(y_sample[:10])\n",
    "y_sample = np.eye(num_classes)[y_sample]#数字编码 to one hot, 牛逼\n",
    "\n",
    "\n",
    "print(\"# of sample is: \", y_sample.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=2.4192, Training Accuracy=0.219\n",
      "Step 50, Minibatch Loss=0.7988, Training Accuracy=0.742\n",
      "Step 100, Minibatch Loss=0.4153, Training Accuracy=0.867\n",
      "Step 150, Minibatch Loss=0.2280, Training Accuracy=0.938\n",
      "Step 200, Minibatch Loss=0.1534, Training Accuracy=0.961\n",
      "Step 250, Minibatch Loss=0.1091, Training Accuracy=0.969\n",
      "Step 300, Minibatch Loss=0.1011, Training Accuracy=0.961\n",
      "Step 350, Minibatch Loss=0.1136, Training Accuracy=0.961\n",
      "Step 400, Minibatch Loss=0.1346, Training Accuracy=0.953\n",
      "Step 450, Minibatch Loss=0.0485, Training Accuracy=0.992\n",
      "Step 500, Minibatch Loss=0.0626, Training Accuracy=0.977\n",
      "Step 550, Minibatch Loss=0.0359, Training Accuracy=0.992\n",
      "Step 600, Minibatch Loss=0.0445, Training Accuracy=0.977\n",
      "test accuracy over 20 batches is : 0.983984375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n结论，把训练中的权重A去掉似乎没什么大的影响，应该是因为大部分分类都是准确的，在准确的情况下其他类概率极低\\n加不加权意义不大\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                        A: np.ones([batch_size, num_classes]), keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))\n",
    "\"\"\"\n",
    "结论，把训练中的权重A去掉似乎没什么大的影响，应该是因为大部分分类都是准确的，在准确的情况下其他类概率极低\n",
    "加不加权意义不大\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=3.1488, Training Accuracy=0.328\n",
      "Step 50, Minibatch Loss=0.1469, Training Accuracy=0.953\n",
      "Step 100, Minibatch Loss=0.1087, Training Accuracy=0.961\n",
      "Step 150, Minibatch Loss=0.0431, Training Accuracy=0.984\n",
      "Step 200, Minibatch Loss=0.0669, Training Accuracy=0.969\n",
      "Step 250, Minibatch Loss=0.0057, Training Accuracy=1.000\n",
      "Step 300, Minibatch Loss=0.0067, Training Accuracy=1.000\n",
      "Step 350, Minibatch Loss=0.0130, Training Accuracy=1.000\n",
      "Step 400, Minibatch Loss=0.0027, Training Accuracy=1.000\n",
      "Step 450, Minibatch Loss=0.0018, Training Accuracy=1.000\n",
      "Step 500, Minibatch Loss=0.0154, Training Accuracy=0.992\n",
      "Step 550, Minibatch Loss=0.0112, Training Accuracy=0.992\n",
      "Step 600, Minibatch Loss=0.0044, Training Accuracy=1.000\n",
      "test accuracy over 20 batches is : 0.98046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n这种情况我们称之为没有什么显著区别\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对比一下随机取样的, 用测试集的1w个样本代替随机抽样\n",
    "#用val_data来做验证\n",
    "\n",
    "\n",
    "A_s = np.ones([10000, num_classes])\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y, batch_A = get_batch_from_X(X_test, y_test, A_s, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op,feed_dict={X: batch_x, Y: batch_y, A: batch_A, keep_prob: dropout})\n",
    "        #print(np.shape(batch_x),np.shape(batch_y))\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,Y: batch_y,A: batch_A, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "\n",
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_val[i*batch_size : (i+1)*batch_size,:],Y: y_val[i*batch_size : (i+1)*batch_size,:], \n",
    "                                        A: np.ones([batch_size, num_classes]), keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))\n",
    "    \n",
    "\"\"\"\n",
    "这种情况我们称之为没有什么显著区别\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-5a9511756e2f>:80: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#然后再来测试一下softmax的普通网络,首先是以刚才的sample为data\n",
    "\n",
    "learning_rate = 0.001 #学习率 \n",
    "batch_size = 128 #批大小\n",
    "num_steps = 500 #使用的样本数量\n",
    "display_step = 50 #显示间隔\n",
    "\n",
    "num_input = 784 #image shape:28*28\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 #用于随机丢弃，防止过拟\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "#创建模型\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "pred = tf.argmax(prediction, 1)\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(pred, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1,num_steps+1):\n",
    "        batch_x, batch_y, A_ = get_batch_from_X(X_sample, y_sample, A_sample, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=116257.7812, Training Accuracy=0.141\n",
      "Step 50, Minibatch Loss=11684.4277, Training Accuracy=0.391\n",
      "Step 100, Minibatch Loss=5451.6616, Training Accuracy=0.602\n",
      "Step 150, Minibatch Loss=4455.7188, Training Accuracy=0.633\n",
      "Step 200, Minibatch Loss=3002.9951, Training Accuracy=0.734\n",
      "Step 250, Minibatch Loss=1931.1198, Training Accuracy=0.750\n",
      "Step 300, Minibatch Loss=1696.3761, Training Accuracy=0.805\n",
      "Step 350, Minibatch Loss=1672.2264, Training Accuracy=0.781\n",
      "Step 400, Minibatch Loss=866.7910, Training Accuracy=0.883\n",
      "Step 450, Minibatch Loss=758.6470, Training Accuracy=0.867\n",
      "Step 500, Minibatch Loss=571.1488, Training Accuracy=0.914\n",
      "test accuracy over 20 batches is : 0.909375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n我们的sample在softmax的cnn上效果不是很好\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                         keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))\n",
    "\"\"\"\n",
    "我们的sample在softmax的cnn上效果不是很好\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8741, 784)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=53995.3438, Training Accuracy=0.109\n",
      "Step 50, Minibatch Loss=3166.9705, Training Accuracy=0.789\n",
      "Step 100, Minibatch Loss=1451.3317, Training Accuracy=0.898\n",
      "Step 150, Minibatch Loss=1777.3933, Training Accuracy=0.914\n",
      "Step 200, Minibatch Loss=788.7242, Training Accuracy=0.953\n",
      "Step 250, Minibatch Loss=1242.3369, Training Accuracy=0.906\n",
      "Step 300, Minibatch Loss=1117.7108, Training Accuracy=0.906\n",
      "Step 350, Minibatch Loss=270.5781, Training Accuracy=0.961\n",
      "Step 400, Minibatch Loss=414.4936, Training Accuracy=0.953\n",
      "Step 450, Minibatch Loss=1155.4604, Training Accuracy=0.914\n",
      "Step 500, Minibatch Loss=464.8482, Training Accuracy=0.969\n",
      "Step 550, Minibatch Loss=607.9376, Training Accuracy=0.938\n",
      "Step 600, Minibatch Loss=458.8017, Training Accuracy=0.938\n",
      "test accuracy over 20 batches is : 0.9421875\n"
     ]
    }
   ],
   "source": [
    "##最后来测试一下softmax方法在整个train_data上的表现\n",
    "learning_rate = 0.002 #学习率 \n",
    "batch_size = 128 #批大小\n",
    "num_steps = 600 #使用的样本数量\n",
    "display_step = 50 #显示间隔\n",
    "\n",
    "def get_batch_from_X1(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch and return the corresponding X_batch, y_batch, A_batch\n",
    "    \"\"\"\n",
    "    data_size = X.shape[0]\n",
    "    idx = np.random.permutation(data_size)\n",
    "    X_random = X[idx]\n",
    "    X_batch = X_random[:batch_size, :]\n",
    "    y_random = y[idx]\n",
    "    y_batch = y_random[:batch_size, :]\n",
    "\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1,num_steps+1):\n",
    "        batch_x, batch_y = get_batch_from_X1(X_train, y_train, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n",
    "            \n",
    "            \n",
    "            \n",
    "test_accuracy = np.zeros(20)\n",
    "with tf.Session() as sess:\n",
    "    train()\n",
    "    for i in range(20):\n",
    "        test_accuracy[i] = sess.run(accuracy, feed_dict={X: X_test[i*batch_size : (i+1)*batch_size,:],Y: y_test[i*batch_size : (i+1)*batch_size,:], \n",
    "                                         keep_prob: 1.0})\n",
    "    print(\"test accuracy over 20 batches is :\", np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
